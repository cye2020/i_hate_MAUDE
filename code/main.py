from typing import List, Set, Iterator
import requests
import zipfile
import io
import ijson
from tqdm import tqdm
import time
import pyarrow as pa
import pyarrow.parquet as pq
import os
import json

SEARCH_URL = 'https://api.fda.gov/download.json'


def search_download_url(start: int, end: int) -> List[str]:
    """ë‹¤ìš´ë¡œë“œ URL ëª©ë¡ ì¡°íšŒ"""
    response = requests.get(SEARCH_URL).json()
    partitions = response['results']['device']['event']['partitions']
    
    urls = []
    for item in partitions:
        first = item['display_name'].split()[0]
        if first.isdigit() and start <= int(first) <= end:
            urls.append(item["file"])
    return urls


def stream_records_from_url(url: str) -> Iterator[dict]:
    """URLì—ì„œ ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë ˆì½”ë“œ yield (ë””ìŠ¤í¬ ì‚¬ìš© ì—†ìŒ)"""
    # 1. ZIP ë‹¤ìš´ë¡œë“œ (ìŠ¤íŠ¸ë¦¬ë°)
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        total = int(r.headers.get("Content-Length", 0))
        
        zip_buffer = io.BytesIO()
        for chunk in tqdm(
            r.iter_content(chunk_size=8 * 1024 * 1024),
            total=total // (8 * 1024 * 1024),
            desc=f"ë‹¤ìš´ë¡œë“œ {url.split('/')[-1]}",
            leave=False
        ):
            zip_buffer.write(chunk)
        
        zip_buffer.seek(0)
    
    # 2. ZIP ì••ì¶• í•´ì œ ë° JSON ìŠ¤íŠ¸ë¦¬ë°
    with zipfile.ZipFile(zip_buffer, 'r') as z:
        json_file = [n for n in z.namelist() if n.endswith(".json")][0]
        
        with z.open(json_file) as f:
            # ijsonìœ¼ë¡œ results ë°°ì—´ì˜ ê° í•­ëª©ì„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹±
            parser = ijson.items(f, 'results.item')
            
            for record in parser:
                yield record
    
    # ë©”ëª¨ë¦¬ í•´ì œ
    del zip_buffer


def flatten_dict(nested_dict, parent_key='', sep='_'):
    """ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ í‰íƒ„í™”"""
    items = []
    
    for k, v in nested_dict.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        elif isinstance(v, list) and v and isinstance(v[0], dict):
            for i, item in enumerate(v):
                items.extend(flatten_dict(item, f"{new_key}_{i}", sep=sep).items())
        else:
            items.append((new_key, v))
    
    return dict(items)


def clean_empty_arrays(obj):
    """ë¹ˆ ê°’ ì •ë¦¬"""
    if isinstance(obj, dict):
        return {k: clean_empty_arrays(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        if obj == [""]:
            return None
        return [clean_empty_arrays(item) for item in obj]
    elif obj == "":
        return None
    return obj


def extract_columns_from_record(record: dict) -> Set[str]:
    """ë‹¨ì¼ ë ˆì½”ë“œì—ì„œ ì»¬ëŸ¼ ì¶”ì¶œ"""
    cleaned = clean_empty_arrays(record)
    flattened = flatten_dict(cleaned)
    return set(flattened.keys())


def normalize_record(record: dict, all_columns: List[str]) -> dict:
    """ë ˆì½”ë“œë¥¼ ì •ê·œí™” (ëª¨ë“  ì»¬ëŸ¼ í¬í•¨)"""
    cleaned = clean_empty_arrays(record)
    flattened = flatten_dict(cleaned)
    
    normalized = {}
    for col in all_columns:
        val = flattened.get(col, None)
        normalized[col] = str(val) if val is not None else None
    
    return normalized


def pass1_collect_schema(urls: List[str], schema_file: str = '.schema_cache.json') -> List[str]:
    """Pass 1: ëª¨ë“  URLì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ë©° ì „ì²´ ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘"""
    print("\n=== Pass 1: ì „ì²´ ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘ ===")
    all_columns = set()
    total_records = 0
    
    for url in urls:
        file_columns = set()
        record_count = 0
        
        print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {url.split('/')[-1]}")
        
        try:
            for record in stream_records_from_url(url):
                columns = extract_columns_from_record(record)
                file_columns.update(columns)
                record_count += 1
                
                # ì§„í–‰ìƒí™© ì¶œë ¥ (1000ê°œë§ˆë‹¤)
                if record_count % 1000 == 0:
                    print(f"  â”œâ”€ {record_count:,}ê°œ ë ˆì½”ë“œ ìŠ¤ìº”...", end='\r')
            
            all_columns.update(file_columns)
            total_records += record_count
            
            print(f"  âœ“ {record_count:,}ê°œ ë ˆì½”ë“œ, {len(file_columns):,}ê°œ ì»¬ëŸ¼ ë°œê²¬")
            
        except Exception as e:
            print(f"  âœ— ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    schema_columns = sorted(all_columns)
    
    # ìŠ¤í‚¤ë§ˆë¥¼ ì‘ì€ íŒŒì¼ë¡œ ì €ì¥ (ì¬ì‹œì‘ ì‹œ ì¬ì‚¬ìš© ê°€ëŠ¥)
    with open(schema_file, 'w') as f:
        json.dump(schema_columns, f)
    
    print(f"\nâœ… ì´ {total_records:,}ê°œ ë ˆì½”ë“œ, {len(schema_columns):,}ê°œ ê³ ìœ  ì»¬ëŸ¼ ë°œê²¬")
    print(f"ğŸ“ ìŠ¤í‚¤ë§ˆ ì €ì¥: {schema_file}\n")
    
    return schema_columns


def pass2_convert_to_parquet(
    urls: List[str], 
    schema_columns: List[str], 
    output_file: str,
    chunk_size: int = 5000
):
    """Pass 2: ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ìœ¼ë¡œ Parquet ë³€í™˜"""
    print("=== Pass 2: Parquet ë³€í™˜ ===")
    
    schema = pa.schema([(col, pa.string()) for col in schema_columns])
    writer = pq.ParquetWriter(output_file, schema, compression='zstd')
    
    total_written = 0
    buffer = []
    
    for url in urls:
        print(f"\nğŸ“„ ë³€í™˜ ì¤‘: {url.split('/')[-1]}")
        record_count = 0
        
        try:
            for record in stream_records_from_url(url):
                normalized = normalize_record(record, schema_columns)
                buffer.append(normalized)
                record_count += 1
                
                # ë²„í¼ê°€ ì°¨ë©´ ì“°ê¸°
                if len(buffer) >= chunk_size:
                    table = pa.Table.from_pylist(buffer, schema=schema)
                    writer.write_table(table)
                    total_written += len(buffer)
                    buffer = []
                    
                    print(f"  â”œâ”€ {total_written:,}ê°œ ë ˆì½”ë“œ ì €ì¥...", end='\r')
            
            print(f"  âœ“ {record_count:,}ê°œ ë ˆì½”ë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"  âœ— ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    # ë‚¨ì€ ë ˆì½”ë“œ ì²˜ë¦¬
    if buffer:
        table = pa.Table.from_pylist(buffer, schema=schema)
        writer.write_table(table)
        total_written += len(buffer)
        buffer = []
    
    writer.close()
    
    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)
    print(f"\nâœ… ì™„ë£Œ! {total_written:,}ê°œ ë ˆì½”ë“œë¥¼ {output_file}ì— ì €ì¥")
    print(f"ğŸ“¦ íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")


def process_fda_data_streaming(
    start: int, 
    end: int, 
    output_file: str = 'output.parquet',
    schema_file: str = '.schema_cache.json',
    skip_pass1: bool = False
):
    """
    ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ì™„ì „ ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
    
    Args:
        start: ì‹œì‘ ì—°ë„
        end: ì¢…ë£Œ ì—°ë„
        output_file: ì¶œë ¥ Parquet íŒŒì¼ëª…
        schema_file: ìŠ¤í‚¤ë§ˆ ìºì‹œ íŒŒì¼ëª…
        skip_pass1: Trueë©´ ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ ì¬ì‚¬ìš© (Pass 1 ê±´ë„ˆë›°ê¸°)
    """
    start_time = time.time()
    
    # 1. URL ìˆ˜ì§‘
    print("ğŸ” ë‹¤ìš´ë¡œë“œ URL ê²€ìƒ‰ ì¤‘...")
    urls = search_download_url(start, end)
    print(f"ì°¾ì€ URL: {len(urls)}ê°œ\n")
    
    if not urls:
        print("âŒ ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. Pass 1: ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘ (ë˜ëŠ” ìºì‹œ ë¡œë“œ)
    if skip_pass1 and os.path.exists(schema_file):
        print(f"â™»ï¸  ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ ë¡œë“œ: {schema_file}")
        with open(schema_file, 'r') as f:
            schema_columns = json.load(f)
        print(f"âœ… {len(schema_columns):,}ê°œ ì»¬ëŸ¼ ë¡œë“œ ì™„ë£Œ\n")
    else:
        schema_columns = pass1_collect_schema(urls, schema_file)
    
    # 3. Pass 2: Parquet ë³€í™˜
    pass2_convert_to_parquet(urls, schema_columns, output_file)
    
    total_time = time.time() - start_time
    print(f"\nâ±ï¸  ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")


if __name__ == '__main__':   
    # ì²˜ìŒ ì‹¤í–‰ (Pass 1 + Pass 2)
    process_fda_data_streaming(
        start=2024,
        end=2024,
        output_file='output.parquet',
        skip_pass1=False  # False: ìŠ¤í‚¤ë§ˆ ìƒˆë¡œ ìˆ˜ì§‘
    )
    
    # ë§Œì•½ ì¤‘ê°„ì— ì¤‘ë‹¨ë˜ì—ˆë‹¤ë©´, ìŠ¤í‚¤ë§ˆ ì¬ì‚¬ìš©í•´ì„œ Pass 2ë§Œ ì‹¤í–‰
    # process_fda_data_streaming(
    #     start=2024,
    #     end=2024,
    #     output_file='output.parquet',
    #     skip_pass1=True  # True: ê¸°ì¡´ .schema_cache.json ì¬ì‚¬ìš©
    # )